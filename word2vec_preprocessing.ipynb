{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_preprocessing",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wovhDeN01cxS",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing for Word2vec\n",
        "This notebook is used to preprocess the input sentences to be embedded using Word2vec. By preprocessing the input sentences, we aim to increase the embedding coverage and reduce the number of unknown embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJJAJtK4eHX",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_uLZPs08RDx",
        "colab_type": "code",
        "outputId": "9b06225d-bbb2-4c90-8563-35b6b64a6bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install tqdm --upgrade\n",
        "!pip install pandas\n",
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.43.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu0jy6So6MA_",
        "colab_type": "text"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ce4GPUY6zHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EI3iERz4sc2",
        "colab_type": "text"
      },
      "source": [
        "This cell loads the Word2vec embeddings pretrained on the Google News dataset. In order ro run this cell, you need to have downloaded [this file](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?usp=sharing) to your Google Drive and replace the `<YOUR_ID_HERE>` with the ID of your file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuveDUsR-SE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "english = drive.CreateFile({'id': '<YOUR_ID_HERE>'})\n",
        "english.GetContentFile('GoogleNews-vectors-negative300.bin.gz')\n",
        "\n",
        "embeddings_index = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xKkhLEy6O5P",
        "colab_type": "text"
      },
      "source": [
        "Loading the input sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glmOSANG70J5",
        "colab_type": "code",
        "outputId": "44ea185d-c997-416e-c816-8d8c3268520f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def load_data(file):\n",
        "    with open(file, 'r') as f:\n",
        "        return f.readlines()\n",
        "    \n",
        "train_en = pd.DataFrame(load_data(\"train.ende.src\"), columns=[\"sentence\"])\n",
        "dev_en = pd.DataFrame(load_data(\"dev.ende.src\"), columns=[\"sentence\"])\n",
        "test_en = pd.DataFrame(load_data(\"test.ende.src\"), columns=[\"sentence\"])\n",
        "train_de = pd.DataFrame(load_data(\"train.ende.mt\"), columns=[\"sentence\"])\n",
        "dev_de = pd.DataFrame(load_data(\"dev.ende.mt\"), columns=[\"sentence\"])\n",
        "test_de = pd.DataFrame(load_data(\"test.ende.mt\"), columns=[\"sentence\"])\n",
        "print(\"Train shape: \", train_en.shape)\n",
        "print(\"Test shape: \", test_en.shape)\n",
        "print(train_en.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (7000, 1)\n",
            "Test shape :  (1000, 1)\n",
            "                                            sentence\n",
            "0  José Ortega y Gasset visited Husserl at Freibu...\n",
            "1  However, a disappointing ninth in China meant ...\n",
            "2  In his diary, Chase wrote that the release of ...\n",
            "3  Heavy arquebuses mounted on wagons were called...\n",
            "4  Once North Pacific salmon die off after spawni...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpuM78ZI6WGy",
        "colab_type": "text"
      },
      "source": [
        "Builds the vocabulary from the input sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVGNyLx9_uJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(sentences, verbose =  True):\n",
        "    \"\"\"\n",
        "    :param sentences: list of list of words\n",
        "    :return: dictionary of words and their count\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUagA0Wv-CnR",
        "colab_type": "code",
        "outputId": "fee09736-8665-477a-c2f1-2f65e97adf59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "sentences = train_en[\"sentence\"].progress_apply(lambda x: x.split()).values\n",
        "vocab = build_vocab(sentences)\n",
        "print({k: vocab[k] for k in list(vocab)[:10]})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 218865.33it/s]\n",
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 146596.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'José': 5, 'Ortega': 1, 'y': 6, 'Gasset': 1, 'visited': 8, 'Husserl': 1, 'at': 491, 'Freiburg': 1, 'in': 2079, '1934.': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVywLGXs6a6r",
        "colab_type": "text"
      },
      "source": [
        "Checks the percentage of words in the vocabulary of the input data which currently have Word2vec embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFPzNGEa-zPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator \n",
        "\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75cmz_9r-13f",
        "colab_type": "code",
        "outputId": "b7c6b3b0-44b3-4cee-cfc1-0e975579a89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/33774 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 3731/33774 [00:00<00:00, 37290.92it/s]\u001b[A\n",
            " 26%|██▌       | 8816/33774 [00:00<00:00, 40529.08it/s]\u001b[A\n",
            " 45%|████▍     | 15089/33774 [00:00<00:00, 45342.31it/s]\u001b[A\n",
            " 63%|██████▎   | 21224/33774 [00:00<00:00, 49192.67it/s]\u001b[A\n",
            "100%|██████████| 33774/33774 [00:00<00:00, 64800.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 59.01% of vocab\n",
            "Found embeddings for  71.10% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie6AomnD6xn5",
        "colab_type": "text"
      },
      "source": [
        "Prints the most common words in the vocabulary without an embedding so we can conduct cleaning of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNmhsASH-4y-",
        "colab_type": "code",
        "outputId": "3943e894-3634-4f37-f3d0-24dfcbd59701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "oov[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 3871),\n",
              " ('of', 2774),\n",
              " ('to', 1710),\n",
              " ('a', 1500),\n",
              " ('However,', 84),\n",
              " ('him.', 48),\n",
              " ('BC.', 34),\n",
              " ('\"The', 33),\n",
              " ('12', 30),\n",
              " ('10', 29)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddsWxLK66zHn",
        "colab_type": "text"
      },
      "source": [
        "Removes the punctuation from the data for which there are no embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCro5gkM-9Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(x):\n",
        "\n",
        "    x = str(x)\n",
        "    for punct in \"/-'\":\n",
        "        x = x.replace(punct, ' ')\n",
        "    for punct in '&':\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
        "        x = x.replace(punct, '')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZFBWtzC_DJD",
        "colab_type": "code",
        "outputId": "bae30908-3523-4e62-a68e-475e95336305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "train_en[\"sentence\"] = train_en[\"sentence\"].progress_apply(lambda x: clean_text(x))\n",
        "dev_en[\"sentence\"] = dev_en[\"sentence\"].progress_apply(lambda x: clean_text(x))\n",
        "test_en[\"sentence\"] = test_en[\"sentence\"].progress_apply(lambda x: clean_text(x))\n",
        "train_de[\"sentence\"] = train_de[\"sentence\"].progress_apply(lambda x: clean_text(x))\n",
        "dev_de[\"sentence\"] = dev_de[\"sentence\"].progress_apply(lambda x: clean_text(x))\n",
        "test_de[\"sentence\"] = test_de[\"sentence\"].progress_apply(lambda x: clean_text(x))\n",
        "sentences = train_en[\"sentence\"].apply(lambda x: x.split())\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 92806.36it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 64246.06it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 62894.43it/s]\n",
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 89403.01it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 53953.67it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 58469.42it/s]\n",
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 153147.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj3CF7wq_PLC",
        "colab_type": "code",
        "outputId": "468b98b0-c862-4c1e-a8fb-7486158f70c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/27616 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 27616/27616 [00:00<00:00, 195856.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 89.66% of vocab\n",
            "Found embeddings for  84.90% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wPkMJgh67g2",
        "colab_type": "text"
      },
      "source": [
        "Removes the punctuation from the data for which there are no embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxnyixcF_P-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_numbers(x):\n",
        "\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn7vJYG-_eQD",
        "colab_type": "code",
        "outputId": "69df4241-680b-49a2-fc2b-cfcb1d7e13a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "train_en[\"sentence\"] = train_en[\"sentence\"].progress_apply(lambda x: clean_numbers(x))\n",
        "dev_en[\"sentence\"] = dev_en[\"sentence\"].progress_apply(lambda x: clean_numbers(x))\n",
        "test_en[\"sentence\"] = test_en[\"sentence\"].progress_apply(lambda x: clean_numbers(x))\n",
        "train_de[\"sentence\"] = train_de[\"sentence\"].progress_apply(lambda x: clean_numbers(x))\n",
        "dev_de[\"sentence\"] = dev_de[\"sentence\"].progress_apply(lambda x: clean_numbers(x))\n",
        "test_de[\"sentence\"] = test_de[\"sentence\"].progress_apply(lambda x: clean_numbers(x))\n",
        "sentences = train_en[\"sentence\"].progress_apply(lambda x: x.split())\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/7000 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 63012.54it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 44207.34it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 55572.83it/s]\n",
            "\n",
            "  0%|          | 0/7000 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 59824.01it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 45606.12it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 44949.25it/s]\n",
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 206273.38it/s]\n",
            "\n",
            "100%|██████████| 7000/7000 [00:00<00:00, 173996.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cpI_WLF_kZB",
        "colab_type": "code",
        "outputId": "4308c078-449f-4220-adcb-5ab7d4deaa5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/26770 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 26770/26770 [00:00<00:00, 253600.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 92.63% of vocab\n",
            "Found embeddings for  87.75% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5nNXS7K7CRk",
        "colab_type": "text"
      },
      "source": [
        "Exports all the data, having improved embedding coverage from 71.1% to 87.75%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdoKaumCA81y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.savetxt('processed/train.ende.src', train_en[\"sentence\"].values, fmt = \"%s\", newline='')\n",
        "np.savetxt('processed/dev.ende.src', dev_en[\"sentence\"].values, fmt = \"%s\", newline='')\n",
        "np.savetxt('processed/test.ende.src', test_en[\"sentence\"].values, fmt = \"%s\", newline='')\n",
        "np.savetxt('processed/train.ende.mt', train_de[\"sentence\"].values, fmt = \"%s\", newline='')\n",
        "np.savetxt('processed/dev.ende.mt', dev_de[\"sentence\"].values, fmt = \"%s\", newline='')\n",
        "np.savetxt('processed/test.ende.mt', test_de[\"sentence\"].values, fmt = \"%s\", newline='')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}