{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "statistical_regressors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYohFO9yPUeU",
        "colab_type": "text"
      },
      "source": [
        "###Word Embeddings\n",
        "For the first attempt, we started by using **spacy** word embeddings for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKrmhkK4P47-",
        "colab_type": "text"
      },
      "source": [
        "**Spacy**\n",
        "\n",
        "The following cells set up spacy. We use the `en_core_web_md` and `de_web_news_md` models to generate word embeddings.\n",
        "\n",
        "The runtime needs to be restarted after installing both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9EujpaPPDoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download de_core_news_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsOu6IHQbHh",
        "colab_type": "text"
      },
      "source": [
        "### Imports\n",
        "\n",
        "The following cell imports all the necessary libraries and loads the `en_core_web_md` and `de_web_news_md` models using **spacy**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrTucRFQOZb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "nlp_english = spacy.load(\"en_core_web_md\")\n",
        "nlp_german = spacy.load(\"de_core_news_md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ouXsSMXcJEY",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "\n",
        "Loads the English sentences with their corresponding German translations, as well as the translation scores provided. This dataset is used for *training*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXVpvsk3OZb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/en-de/train.ende.src', 'r') as f:\n",
        "    train_english = []\n",
        "    for line in f:\n",
        "        train_english.append(line)\n",
        "\n",
        "with open('data/en-de/train.ende.mt', 'r') as f:\n",
        "    train_german = []\n",
        "    for line in f:\n",
        "        train_german.append(line)\n",
        "\n",
        "with open('data/en-de/train.ende.scores', 'r') as f:\n",
        "    train_scores = []\n",
        "    for line in f:\n",
        "        train_scores.append(line)\n",
        "\n",
        "train_scores = [score.replace('\\n', '') for score in train_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crtdgVR4cqT5",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess\n",
        "\n",
        "Converts the *training* input sentences and translations to a sequence of **spacy** `Token` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk4tpLmuOZcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_english = list(nlp_english.pipe(train_english, batch_size=32, n_threads=7))\n",
        "doc_german = list(nlp_german.pipe(train_german, batch_size=32, n_threads=7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTVXoZR_et_z",
        "colab_type": "text"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "#### Word Embeddings Method\n",
        "\n",
        "Uses **spacy** to vectorise the input list of sentences to a **numpy** array of dimension `[MAX NUMBER OF SENTENCES, MAX SENTENCE LENGTH, EMBEDDING DIMENSION]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dmYqqN7OZcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_sentences(doc, nlp, max_words):\n",
        "    result = []\n",
        "    unknown_vector = nlp.vocab['unk'].vector\n",
        "    \n",
        "    result = np.zeros((len(doc), 76, len(unknown_vector)))\n",
        "\n",
        "    for sentence_index, sentence in enumerate(doc):\n",
        "        tokens = []\n",
        "        token_index = 0 \n",
        "        for sent in sentence.sents:\n",
        "            for i in range(len(sent)):\n",
        "                token = sent[i]\n",
        "\n",
        "                if token.has_vector:\n",
        "                    result[sentence_index, token_index] = token.vector\n",
        "                else:\n",
        "                    result[sentence_index, token_index] = unknown_vector\n",
        "                \n",
        "                token_index += 1\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l00sVEV2gaaU",
        "colab_type": "text"
      },
      "source": [
        "#### English and German Embeddings\n",
        "\n",
        "Calculates the maximum length of the *train* input sentences and then converts the input sentences into word embeddings using the method above. Then converts the 3-dimensional word embeddings into 2-dimensional embeddings by averaging the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-MdDQ1UOZcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words_english = max(map(len, doc_english))\n",
        "max_words_german = max(map(len, doc_german))\n",
        "\n",
        "max_words = max(max_words_english, max_words_german)\n",
        "\n",
        "english_embeddings = embed_sentences(doc_english, nlp_english, max_words)\n",
        "german_embeddings = embed_sentences(doc_german, nlp_german, max_words)\n",
        "\n",
        "english_embeddings = np.average(english_embeddings, axis=1)\n",
        "german_embeddings = np.average(german_embeddings, axis=1)\n",
        "\n",
        "word_embeddings = np.concatenate((english_embeddings, german_embeddings), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj4_f33qi1mz",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "\n",
        "Loads the English sentences with their corresponding German translations, as well as the translation scores provided. This dataset is used for *testing*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5d3pRH-OZca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/en-de/dev.ende.src', 'r') as f:\n",
        "    test_english = []\n",
        "    for line in f:\n",
        "        test_english.append(line)\n",
        "\n",
        "with open('data/en-de/dev.ende.mt', 'r') as f:\n",
        "    test_german = []\n",
        "    for line in f:\n",
        "        test_german.append(line)\n",
        "        \n",
        "with open('data/en-de/dev.ende.scores', 'r') as f:\n",
        "    test_scores = []\n",
        "    for line in f:\n",
        "        test_scores.append(line)\n",
        "\n",
        "test_scores = [float(score.replace('\\n', '')) for score in test_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCuFMdaln22",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess\n",
        "\n",
        "Converts the *testing* input sentences and translations to a sequence of **spacy** `Token` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjnl0RR0OZcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_english = list(nlp_english.pipe(test_english, batch_size=32, n_threads=7))\n",
        "doc_german = list(nlp_german.pipe(test_german, batch_size=32, n_threads=7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5zVwSBKl8lz",
        "colab_type": "text"
      },
      "source": [
        "#### English and German Embeddings\n",
        "\n",
        "Calculates the maximum length of the *test* input sentences and then converts the input sentences into word embeddings using the same method as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPtPqUhPOZce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words_english = max(map(len, doc_english))\n",
        "max_words_german = max(map(len, doc_german))\n",
        "\n",
        "max_words = max(max_words_english, max_words_german)\n",
        "\n",
        "english_embeddings = embed_sentences(doc_english, nlp_english, max_words)\n",
        "german_embeddings = embed_sentences(doc_german, nlp_german, max_words)\n",
        "\n",
        "english_embeddings = np.average(english_embeddings, axis=1)\n",
        "german_embeddings = np.average(german_embeddings, axis=1)\n",
        "\n",
        "word_embeddings_test = np.concatenate((english_embeddings, german_embeddings), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01JRp9YiQdDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_scores = np.array(train_scores)\n",
        "test_scores = np.array(test_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhlsePlYom2f",
        "colab_type": "text"
      },
      "source": [
        "### Statistical Machine Learning Models\n",
        "\n",
        "Uses different regression models from **sklearn** to predict the accuracy of the translation by training this model on the averaged word embeddings and the scores of the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPovnjklOZch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, Lasso, LassoLars\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import math\n",
        "\n",
        "regression_models = [\n",
        "                     LinearRegression(),\n",
        "                     Ridge(),\n",
        "                     BayesianRidge(),\n",
        "                     SVR(),\n",
        "]\n",
        "\n",
        "best_mae = 1000\n",
        "best_model = None\n",
        "best_pearson = -1000\n",
        "best_rmse = 1000\n",
        "\n",
        "for index, model in enumerate(regression_models):\n",
        "    print(f'START TRAINING MODEL NO. {index + 1}...')\n",
        "    model.fit(word_embeddings, train_scores)\n",
        "    prediction = model.predict(word_embeddings_test)\n",
        "    mae = mean_absolute_error(test_scores, prediction)\n",
        "    pearson_score = pearsonr(prediction, test_scores)\n",
        "    rmse = math.sqrt(mean_squared_error(test_scores, prediction))\n",
        "    \n",
        "    print(f'Model: {str(model)}\\n')\n",
        "    print(f'Pearson Score: {pearson_score}\\n')\n",
        "    print(f'MAE: {mae}\\n')\n",
        "    print(f'RMSE: {rmse}\\n')\n",
        "\n",
        "    if pearson_score[0] > best_pearson:\n",
        "        best_pearson = pearson_score[0]\n",
        "        best_model = model\n",
        "        best_mae = mae\n",
        "        best_rmse = rmse\n",
        "    print('FINISHED TRAINING.\\n')\n",
        "\n",
        "print(f'Best Model: {str(best_model)}\\n')\n",
        "print(f'Best Pearson Score: {best_pearson}\\n')\n",
        "print(f'Best MAE: {best_mae}\\n')\n",
        "print(f'Best RMSE: {best_rmse}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}