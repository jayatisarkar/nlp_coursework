{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_mwXSvCLkJ3",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3CXii-GMG5N",
        "colab_type": "text"
      },
      "source": [
        "Set up directory structure: \n",
        "\n",
        "*   *data/en-de/* \n",
        "    \n",
        "    Directory containing the datasets: english sentences, translations and scores for training, validation and testing\n",
        "\n",
        "*   *tensors/\\<model\\>/*\n",
        "\n",
        "    Directory where the tensors for the sentences and translations generated with each language model (or custom features) are saved. \n",
        "    \n",
        "    Generating and saving all the tensors once  allows faster loading of the data for training and testing.\n",
        "\n",
        "*   *features*/\n",
        "\n",
        "    Directory where the tensors for the features of the sentences and translations are saved\n",
        "\n",
        "*   *saved_models*/\n",
        "\n",
        "    Directory where the best models are saved\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_2uQexC9Y3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data/en-de/\n",
        "!mkdir tensors/\n",
        "!mkdir tensors/spacy\n",
        "!mkdir tensors/bert\n",
        "!mkdir tensors/word2vec\n",
        "!mkdir tensors/bpemb\n",
        "!mkdir tensors/features\n",
        "!mkdir saved_models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-GtvAZHMWtv",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings\n",
        "We have tried out different word embeddings for our models. In particular, we've used **BERT**, **spaCy** and **word2vec**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZZUSVTRI6iP",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec\n",
        "\n",
        "The following cells setup and load the word2vec models for english and german. The models are quite large, so we have to download them from google drive because a manual file upload fails. In order to generate Word2vec embeddings, you need to have downloaded [this Google News file](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?usp=sharing) and [this German model file](http://cloud.devmount.de/d2bc5672c523b086/) to your Google Drive and replace the `<YOUR_X_ID_HERE>` with the ID of your files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhoESiOFI4ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyDrive\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qOK-ev8Isnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "english = drive.CreateFile({'id': '<YOUR_GERMAN_MODEL_ID_HERE>'})\n",
        "english.GetContentFile('german.model')\n",
        "\n",
        "german = drive.CreateFile({'id': '<YOUR_GOOGLE_NEWS_ID_HERE>'})\n",
        "german.GetContentFile('GoogleNews-vectors-negative300.bin.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAGwZkNdKCSF",
        "colab_type": "text"
      },
      "source": [
        "### spaCy\n",
        "\n",
        "The following cells set up spaCy for GloVe embeddings. We use the `en_core_web_md` and `de_web_news_md` models to generate word embeddings.\n",
        "\n",
        "The runtime needs to be restarted after installing both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DRxG7JtNZCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download de_core_news_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9sDYnuM8FPb",
        "colab_type": "text"
      },
      "source": [
        "### BPemb\n",
        "\n",
        "The following cells install the **bpemb** library for byte pair encoding sub-word embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdfBLQhs7ZbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bpemb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXDXvpdW-BFv",
        "colab_type": "text"
      },
      "source": [
        "### BERT\n",
        "\n",
        "In order to generate the sentence embeddings for BERT, we will use [bert-as-service](https://github.com/hanxiao/bert-as-service) for which you must have downloaded [this BERT model](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip). Follow the instructions on the `README` of [bert-as-service](https://github.com/hanxiao/bert-as-service) using `max_seq_len=None` to generate the embeddings and place these into the `tensors/bert` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH7j39-tcBo5",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwdMGrEi9eAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
        "from keras.layers.core import Lambda, Masking\n",
        "from keras.layers.recurrent import GRU, LSTM\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.callbacks import TerminateOnNaN, ModelCheckpoint, EarlyStopping, \\\n",
        "    TensorBoard\n",
        "import keras.backend as K\n",
        "import keras.layers as L\n",
        "from keras.utils import Sequence, to_categorical, multi_gpu_model\n",
        "from keras import metrics\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyk6UeMNb4Wi",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ousEYBxNdjWk",
        "colab_type": "text"
      },
      "source": [
        "Define global constants used for retrieving file names for saving/loading data to files. Also define constants for needed for preprocessing and building models, such as the dimension of the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9q1m1AlA0Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_dir = \"features\"\n",
        "tensor_dir = \"tensors\"\n",
        "sentences_file = \"sentences_tensor.npy\"\n",
        "translations_file = \"translations_tensor.npy\"\n",
        "sents_files = {\"train\": 'data/en-de/train.ende.src', \"val\": \"data/en-de/dev.ende.src\", \"test\": \"data/en-de/test.ende.src\"}\n",
        "trans_files = {\"train\": \"data/en-de/train.ende.mt\", \"val\": \"data/en-de/dev.ende.mt\", \"test\": \"data/en-de/test.ende.src\"}\n",
        "MAX_WORDS = 60\n",
        "BATCH_SIZE = 100\n",
        "VECTOR_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW2l72nFcNc3",
        "colab_type": "text"
      },
      "source": [
        "## File I/O Utils\n",
        "\n",
        "Functions to read the datasets and scores and load saved tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJBD848u9i2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(file):\n",
        "    with open(file, 'r') as f:\n",
        "        return [x.strip() for x in f.readlines()]\n",
        "\n",
        "\n",
        "def load_scores(scores_file):\n",
        "    scores = load_data(scores_file)\n",
        "    return np.array(scores, dtype=np.float32)\n",
        "\n",
        "\n",
        "def load_tensors(t_dir, tensor_file, model=\"spacy\", dataset=\"train\"):\n",
        "    tensor_file = f'{t_dir}/{model}/{dataset}_{tensor_file}'\n",
        "    return np.load(tensor_file)\n",
        "\n",
        "\n",
        "def load_inputs(dataset=\"train\", model=\"spacy\"):\n",
        "\n",
        "    sentences = load_tensors(tensor_dir, sentences_file, model=model, dataset=dataset)\n",
        "    translations = load_tensors(tensor_dir, translations_file, model=model, dataset=dataset)\n",
        "    \n",
        "    return sentences, translations "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJf5GV-6c51N",
        "colab_type": "text"
      },
      "source": [
        "## word2vec preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciKQlbSbIJI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_word2vec_models():\n",
        "    # Load vectors directly from the file\n",
        "    english_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "    german_model =  KeyedVectors.load_word2vec_format(\"german.model\", binary=True)\n",
        "    return english_model, german_model\n",
        "\n",
        "\n",
        "def preprocess_inputs(word2vec_english_model, word2vec_german_model,\n",
        "                            english_data, german_data):\n",
        "    train_english = load_data(english_data)\n",
        "    train_german = load_data(german_data)\n",
        "\n",
        "    english_embeddings = embed_sentences_w2v(word2vec_english_model, train_english)\n",
        "    german_embeddings = embed_sentences_w2v(word2vec_german_model, train_german)\n",
        "\n",
        "    return english_embeddings, german_embeddings\n",
        "\n",
        "\n",
        "def embed_sentences_w2v(word2vec_model, train, max_words=MAX_WORDS):\n",
        "    result = np.zeros((len(train), max_words, 300))\n",
        "\n",
        "    for sentence_index, sentence in enumerate(train):\n",
        "        for word_index, word in enumerate(sentence.split(' ')):\n",
        "            word = word.strip('\\n')\n",
        "            word = re.sub(r'\\W+', '', word)\n",
        "            try:\n",
        "              result[sentence_index, word_index] = word2vec_model[word]\n",
        "            except:\n",
        "              continue\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_word2vec_vecs(word2vec_english_model, word2vec_german_model, dataset=\"train\", save=True):\n",
        "    # key = \"dev\" if test else \"train\"\n",
        "    sents_f = sents_files[dataset]\n",
        "    translations_f = trans_files[dataset]\n",
        "    sentences = load_data(sents_f)\n",
        "    translations = load_data(translations_f)\n",
        "    sentences_tensor, translations_tensor = preprocess_inputs(word2vec_english_model,\n",
        "                                                                    word2vec_german_model,\n",
        "                                                                    sents_f,\n",
        "                                                                    translations_f)\n",
        "    if save:\n",
        "        sents_file = f\"{dataset}_{sentences_file}\"\n",
        "        trans_file = f\"{dataset}_{translations_file}\"\n",
        "        sents_file = f'{tensor_dir}/{sents_file}'\n",
        "        trans_file = f'{tensor_dir}/{trans_file}'\n",
        "        np.save(sents_file, sentences_tensor)\n",
        "        np.save(trans_file, translations_tensor)\n",
        "\n",
        "    return sentences_tensor, translations_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwNqVwJ2eMQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_english_model, word2vec_german_model = load_word2vec_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6gdeWd9dOOy",
        "colab_type": "text"
      },
      "source": [
        "## spaCy preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH64u0kBD96i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_spacy_models():\n",
        "    nlp_english = spacy.load(\"en_core_web_md\")\n",
        "    nlp_german = spacy.load(\"de_core_news_md\")\n",
        "    return nlp_english, nlp_german\n",
        "\n",
        "def spacy_preprocess_inputs(nlp_english, nlp_german,\n",
        "                            english_data, german_data, max_words=MAX_WORDS):\n",
        "    train_english = load_data(english_data)\n",
        "    train_german = load_data(german_data)\n",
        "\n",
        "    doc_english = get_doc(nlp_english, train_english)\n",
        "    doc_german = get_doc(nlp_german, train_german)\n",
        "\n",
        "    sentences = embed_sentences(doc_english, nlp_english, max_words)\n",
        "    translations = embed_sentences(doc_german, nlp_german, max_words)\n",
        "\n",
        "    return sentences, translations\n",
        "\n",
        "\n",
        "def get_doc(nlp, train):\n",
        "    doc = list(nlp.pipe(train, batch_size=32, n_threads=7))\n",
        "    return doc\n",
        "\n",
        "\n",
        "def embed_sentences(doc, nlp, max_words):\n",
        "    unknown_vector = nlp.vocab['unk'].vector\n",
        "    \n",
        "    result = np.zeros((len(doc), max_words, len(unknown_vector)), dtype=np.float32)\n",
        "\n",
        "    for sentence_index, sentence in enumerate(doc):\n",
        "        token_index = 0\n",
        "        for sent in sentence.sents:\n",
        "            for i in range(len(sent)):\n",
        "                token = sent[i]\n",
        "\n",
        "                if token.has_vector:\n",
        "                    result[sentence_index, token_index] = token.vector\n",
        "                else:\n",
        "                    result[sentence_index, token_index] = unknown_vector\n",
        "\n",
        "                token_index += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_spacy_vecs(nlp_english, nlp_german, dataset=\"train\", save=True):\n",
        "    sents_f = sents_files[dataset]\n",
        "    translations_f = trans_files[dataset]\n",
        "    sentences = load_data(sents_f)\n",
        "    translations = load_data(translations_f)\n",
        "    sentences_tensor, translations_tensor = spacy_preprocess_inputs(nlp_english,\n",
        "                                                                    nlp_german,\n",
        "                                                                    sents_f,\n",
        "                                                                    translations_f)\n",
        "    if save:\n",
        "        sents_file = f\"{dataset}_{sentences_file}\"\n",
        "        trans_file = f\"{dataset}_{translations_file}\"\n",
        "        sents_file = f'{tensor_dir}/spacy/{sents_file}'\n",
        "        trans_file = f'{tensor_dir}/spacy/{trans_file}'\n",
        "        np.save(sents_file, sentences_tensor)\n",
        "        np.save(trans_file, translations_tensor)\n",
        "\n",
        "    return sentences_tensor, translations_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMbw_wQa-grJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load spacy language models only once\n",
        "nlp_english, nlp_german = load_spacy_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73k7lQCe7TJS",
        "colab_type": "text"
      },
      "source": [
        "## BPE preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erU6NeZ47cGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bpemb import BPEmb\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_bp_models():\n",
        "    bpemb_en = BPEmb(lang='en', vs=100000)\n",
        "    bpemb_de = BPEmb(lang='de', vs=100000)\n",
        "    return bpemb_en, bpemb_de\n",
        "\n",
        "\n",
        "def bpe_embed_sentences(bpemb_model, train):\n",
        "    max_byte_encoding_length = -1\n",
        "    max_embedding_dimension = 100\n",
        "    embeddings = []\n",
        "    for sentence in train:\n",
        "      embedding = bpemb_model.embed(sentence)\n",
        "      embeddings.append(embedding)\n",
        "      max_byte_encoding_length = max(max_byte_encoding_length, embedding.shape[0])\n",
        "    \n",
        "    result = np.zeros((len(train), max_byte_encoding_length, max_embedding_dimension))\n",
        "\n",
        "    for sentence_index, embedding in enumerate(embeddings):\n",
        "      for i in range(embedding.shape[0]):\n",
        "        result[sentence_index, i] = embedding[i]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def bpe_preprocess_inputs(bpemb_en, bpemb_de,\n",
        "                            english_data, german_data):\n",
        "    train_english = load_data(english_data)\n",
        "    train_german = load_data(german_data)\n",
        "\n",
        "    english_embeddings = bpe_embed_sentences(bpemb_en, train_english)\n",
        "    german_embeddings = bpe_embed_sentences(bpemb_de, train_german)\n",
        "\n",
        "    sentences = torch.tensor(english_embeddings, dtype=torch.float32)\n",
        "    translations = torch.tensor(german_embeddings, dtype=torch.float32)\n",
        "    return sentences, translations\n",
        "\n",
        "\n",
        "def load_bpe_vecs(bpemb_en, bpemb_de, dataset=\"train\", save=True):\n",
        "    bpemb_en, bpemb_de = load_bp_models()\n",
        "\n",
        "    sents_f = sents_files[dataset]\n",
        "    translations_f = trans_files[dataset]\n",
        "    sentences = load_data(sents_f)\n",
        "    translations = load_data(translations_f)\n",
        "    sentences_tensor, translations_tensor = bpe_preprocess_inputs(bpemb_en,\n",
        "                                                                    bpemb_de,\n",
        "                                                                    sents_f,\n",
        "                                                                    translations_f)\n",
        "    \n",
        "    if save:\n",
        "        sents_file = f\"{dataset}_{sentences_file}\"\n",
        "        trans_file = f\"{dataset}_{translations_file}\"\n",
        "        sents_file = f'{tensor_dir}/bpemb/{sents_file}'\n",
        "        trans_file = f'{tensor_dir}/bpemb/{trans_file}'\n",
        "        np.save(sents_file, sentences_tensor)\n",
        "        np.save(trans_file, translations_tensor)\n",
        "    \n",
        "    return sentences_tensor, translations_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEF0NY517hIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bpemb_en, bpemb_de = load_bp_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MPQrI8DDACX",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFHmmD7hDi6P",
        "colab_type": "text"
      },
      "source": [
        "Extract sentence-level features from sentences:\n",
        "\n",
        "*   Number of tokens in each source sentence\n",
        "*   Number of tokens in each translation\n",
        "*   Ratio of the number tokens in source to translation\n",
        "*   Average source token length\n",
        "*   Average translation token length\n",
        "*   Number of named entities in source\n",
        "*   Number of named entities in translation\n",
        "*   Number of puctuation marks in source\n",
        "*   Number of puctuation marks in translation\n",
        "*   Frequencies of different POS tags in source sentence\n",
        "*   Frequencies of different POS tags in translation\n",
        "*   Frequencies of different fine-grained POS tags in source sentence\n",
        "*   Frequencies of different fine-grained POS tags in translation\n",
        "*   Frequencies of different dependency tags in source sentence\n",
        "*   Frequencies of different dependency tags in translation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd5h-dL7DLZ9",
        "colab_type": "text"
      },
      "source": [
        "Set of all possible tag values for spaCy POS tags and dependency tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEGWGEqYDK3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = {'ADJ': 0, 'ADP': 1, 'ADV': 2, 'AUX': 3, 'CONJ': 4, 'CCONJ': 5, 'DET': 6, 'INTJ': 7, 'NOUN': 8, 'NUM': 9, 'PART': 10, 'PRON': 11, 'PROPN': 12, 'PUNCT': 13, 'SCONJ': 14, 'SYM': 15, 'VERB': 16, 'X': 17, 'EOL': 18, 'SPACE': 19}\n",
        "\n",
        "en_dep = {'acl': 0, 'acomp': 1, 'advcl': 2, 'advmod': 3, 'agent': 4, 'amod': 5, 'appos': 6, 'attr': 7, 'aux': 8, 'auxpass': 9, 'case': 10, 'cc': 11, 'ccomp': 12, 'compound': 13, 'conj': 14, 'cop': 15, 'csubj': 16, 'csubjpass': 17, 'dative': 18, 'dep': 19, 'det': 20, 'dobj': 21, 'expl': 22, 'intj': 23, 'mark': 24, 'meta': 25, 'neg': 26, 'nn': 27, 'nounmod': 28, 'npmod': 29, 'nsubj': 30, 'nsubjpass': 31, 'nummod': 32, 'oprd': 33, 'obj': 34, 'obl': 35, 'parataxis': 36, 'pcomp': 37, 'pobj': 38, 'poss': 39, 'preconj': 40, 'prep': 41, 'prt': 42, 'punct': 43, 'quantmod': 44, 'relcl': 45, 'root': 46, 'xcomp': 47, 'clf': 48, 'discourse': 49, 'dislocated': 50, 'fixed': 51, 'flat': 52, 'goeswith': 53, 'iobj': 54, 'list': 55, 'nmod': 56, 'orphan': 57, 'reparandum': 58, 'vocative': 59, 'npadvmod': 60, 'subtok': 61, 'predet': 62}\n",
        "en_tag = {'$': 0, '``': 1, \"''\": 2, ',': 3, '-LRB-': 4, '-RRB-': 5, '.': 6, ':': 7, 'ADD': 8, 'AFX': 9, 'PRP$': 10, '_SP': 11, 'WP$': 12, 'CC': 13, 'CD': 14, 'DT': 15, 'EX': 16, 'FW': 17, 'GW': 18, 'HYPH': 19, 'IN': 20, 'JJ': 21, 'JJR': 22, 'JJS': 23, 'LS': 24, 'MD': 25, 'NFP': 26, 'NIL': 27, 'NN': 28, 'NNP': 29, 'NNPS': 30, 'NNS': 31, 'PDT': 32, 'POS': 33, 'PRP': 34, 'RB': 35, 'RBR': 36, 'RBS': 37, 'RP': 38, 'SP': 39, 'SYM': 40, 'TO': 41, 'UH': 42, 'VB': 43, 'VBD': 44, 'VBG': 45, 'VBN': 46, 'VBP': 47, 'VBZ': 48, 'WDT': 49, 'WP': 50, 'WRB': 51, 'XX': 52}\n",
        "de_dep = {'ac': 0, 'adc': 1, 'ag': 2, 'ams': 3, 'app': 4, 'avc': 5, 'cc': 6, 'cd': 7, 'cj': 8, 'cm': 9, 'cp': 10, 'cvc': 11, 'da': 12, 'dm': 13, 'ep': 14, 'ju': 15, 'mnr': 16, 'mo': 17, 'ng': 18, 'nk': 19, 'nmc': 20, 'oa': 21, 'oa2': 22, 'oc': 23, 'og': 24, 'op': 25, 'par': 26, 'pd': 27, 'pg': 28, 'ph': 29, 'pm': 30, 'pnc': 31, 'punct': 32, 'rc': 33, 're': 34, 'rs': 35, 'sb': 36, 'sbp': 37, 'sp': 38, 'svp': 39, 'uc': 40, 'vo': 41, 'ROOT': 42, 'root': 43, 'subtok': 44, 'dep': 45}\n",
        "de_tag = {'$(': 0, '$,': 1, '$.': 2, 'ADJA': 3, 'ADJD': 4, 'ADV': 5, 'APPO': 6, 'APPR': 7, 'APPRART': 8, 'APZR': 9, 'ART': 10, 'CARD': 11, 'FM': 12, 'ITJ': 13, 'KOKOM': 14, 'KON': 15, 'KOUI': 16, 'KOUS': 17, 'NE': 18, 'NN': 19, 'NNE': 20, 'PDAT': 21, 'PDS': 22, 'PIAT': 23, 'PIS': 24, 'PPER': 25, 'PPOSAT': 26, 'PPOSS': 27, 'PRELAT': 28, 'PRELS': 29, 'PRF': 30, 'PROAV': 31, 'PTKA': 32, 'PTKANT': 33, 'PTKNEG': 34, 'PTKVZ': 35, 'PTKZU': 36, 'PWAT': 37, 'PWAV': 38, 'PWS': 39, 'TRUNC': 40, 'VAFIN': 41, 'VAIMP': 42, 'VAINF': 43, 'VAPP': 44, 'VMFIN': 45, 'VMINF': 46, 'VMPP': 47, 'VVFIN': 48, 'VVIMP': 49, 'VVINF': 50, 'VVIZU': 51, 'VVPP': 52, 'XY': 53, '_SP': 54}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJU_94IMM6K",
        "colab_type": "text"
      },
      "source": [
        "Functions to extract features from sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfXrmO3cDGKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def num_tokens_in_sent(doc):\n",
        "    return len(doc)\n",
        "\n",
        "def avg_token_len(doc):\n",
        "    return (sum(len(tok.text) for tok in doc) / len(doc))\n",
        "\n",
        "def num_named_entities(doc):\n",
        "    return sum(1 for tok in doc if tok.ent_type_ != '')\n",
        "\n",
        "def num_punctuation_marks(doc):\n",
        "    return sum(1 for tok in doc if tok.is_punct)\n",
        "\n",
        "def pos_tag_freqs(doc):\n",
        "    pos_freqs = np.zeros((len(pos),))\n",
        "    for tok in doc:\n",
        "        pos_idx = pos[tok.pos_]\n",
        "        pos_freqs[pos_idx] += 1\n",
        "    return pos_freqs\n",
        "\n",
        "def fine_grained_pos_tag_freqs(doc, lang='en'):    \n",
        "    tags = en_tag if lang == 'en' else de_tag\n",
        "    fg_pos_freqs = np.zeros((len(tags),))\n",
        "    \n",
        "    for tok in doc:\n",
        "        fg_pos_idx = tags[tok.tag_]\n",
        "        fg_pos_freqs[fg_pos_idx] += 1\n",
        "    return fg_pos_freqs\n",
        "\n",
        "def dependency_tag_freqs(doc, lang='en'):\n",
        "    dep_tags = en_dep if lang == 'en' else de_dep\n",
        "    dep_freqs = np.zeros((len(dep_tags),))\n",
        "    \n",
        "    for tok in doc:\n",
        "        dep_idx = dep_tags[tok.dep_.lower()]\n",
        "        dep_freqs[dep_idx] += 1\n",
        "    return dep_freqs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HQve2mSMDPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(nlp_en, nlp_de, dataset='train', save=True):\n",
        "    sents_f = sents_files[dataset]\n",
        "    translations_f = trans_files[dataset]\n",
        "\n",
        "    sentences = load_data(sents_f)\n",
        "    translations = load_data(translations_f)\n",
        "\n",
        "    sent_docs = get_doc(nlp_en, sentences)\n",
        "    trans_docs = get_doc(nlp_de, translations)\n",
        "\n",
        "\n",
        "    features = [pairwise_features(sent, trans) for sent, trans in zip(sent_docs, trans_docs)]\n",
        "    sent_features, trans_features = tuple(zip(*features))\n",
        "    sentence_features = np.array(sent_features)\n",
        "    translation_features = np.array(trans_features)\n",
        "\n",
        "    if save:\n",
        "        np.save(f'{tensor_dir}/{features_dir}/{dataset}_{sentences_file}', sentence_features)\n",
        "        np.save(f'{tensor_dir}/{features_dir}/{dataset}_{translations_file}', translation_features)\n",
        "    return sentence_features, translation_features\n",
        "\n",
        "def pairwise_features(sentence, translation):  \n",
        "    sentence_features, trans_features = get_features(sentence, lang='en'), get_features(translation, lang='de')\n",
        "    num_sent_tokens = sentence_features[-1]\n",
        "    num_trans_tokens = trans_features[-1]\n",
        "    num_tokens_ratio = num_sent_tokens[0] / num_trans_tokens[0]\n",
        "    sentence_features.append([num_tokens_ratio])\n",
        "    trans_features.append([num_tokens_ratio])\n",
        "\n",
        "    return np.concatenate(sentence_features, axis=0), np.concatenate(trans_features, axis=0)\n",
        "\n",
        "def get_features(sentence, lang='en'):\n",
        "    features = []\n",
        "    features.append(pos_tag_freqs(sentence))\n",
        "    features.append(fine_grained_pos_tag_freqs(sentence, lang=lang))\n",
        "    features.append(dependency_tag_freqs(sentence, lang=lang))\n",
        "    features.append([num_named_entities(sentence)])\n",
        "    features.append([num_punctuation_marks(sentence)])\n",
        "    features.append([avg_token_len(sentence)])\n",
        "    features.append([num_tokens_in_sent(sentence)])\n",
        "    return features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpNy0PWoxw6I",
        "colab_type": "text"
      },
      "source": [
        "## Generating tensors\n",
        "\n",
        "Comment/uncomment the appropriate lines according to which embeddings you would like to generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_J1EhzL-3u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate vectors for training, testing and validation once, saving them so\n",
        "# they can be loaded faster\n",
        "\n",
        "spacy_train_sentences, spacy_train_translations = load_spacy_vecs(nlp_english, nlp_german, dataset=\"train\", save=True)\n",
        "print(\"spacy train generated\")\n",
        "spacy_val_sentences, spacy_val_translations = load_spacy_vecs(nlp_english, nlp_german, dataset=\"val\", save=True)\n",
        "print(\"spacy val generated\")\n",
        "spacy_test_sentences, spacy_test_translations = load_spacy_vecs(nlp_english, nlp_german, dataset=\"test\", save=True)\n",
        "print(\"spacy test generated\")\n",
        "\n",
        "# w2v_train_sentences, w2v_train_translations = load_word2vec_vecs(word2vec_english_model, word2vec_german_model, dataset='train', save=True)\n",
        "# print(\"word2vec train generated\")\n",
        "# w2v_val_sentences, w2v_val_translations = load_word2vec_vecs(word2vec_english_model, word2vec_german_model, dataset='val', save=True)\n",
        "# print(\"word2vec val generated\")\n",
        "# w2v_test_sentences, w2v_test_translations = load_word2vec_vecs(word2vec_english_model, word2vec_german_model, dataset='test', save=True)\n",
        "# print(\"word2vec test generated\")\n",
        "\n",
        "# bpe_train_sentences, bpe_train_translations = load_bpe_vecs(bpemb_en, bpemb_de, dataset=\"train\", save=True)\n",
        "# print(\"bpemb train loaded\")\n",
        "# bpe_val_sentences, bpe_val_translations = load_bpe_vecs(bpemb_en, bpemb_de, dataset=\"val\", save=True)\n",
        "# print(\"bpemb val loaded\")\n",
        "# bpe_test_sentences, bpe_test_translations = load_bpe_vecs(bpemb_en, bpemb_de, dataset=\"test\", save=True)\n",
        "# print(\"bpemb test loaded\")\n",
        "\n",
        "train_sent_features, train_trans_features = extract_features(nlp_english, nlp_german, dataset=\"train\", save=True)\n",
        "print(\"train features generated\")\n",
        "val_sent_features, val_trans_features = extract_features(nlp_english, nlp_german, dataset='val', save=True)\n",
        "print(\"val features generated\")\n",
        "test_sent_features, test_trans_features = extract_features(nlp_english, nlp_german, dataset='test', save=True)\n",
        "print(\"test features generated\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGRQNnXZfNlV",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQUHk-VElaJq",
        "colab_type": "text"
      },
      "source": [
        "### MLP models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foc-yT629yAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mlp_model(architecture):\n",
        "    layers = architecture['layers']\n",
        "    activation = architecture['activation']\n",
        "    num_features = architecture['num_features']\n",
        "\n",
        "    # Inputs are sentence embeddings\n",
        "    sentence = L.Input(shape=(num_features,), name='sentence')\n",
        "    translation = L.Input(shape=(num_features,), name='translation')\n",
        "\n",
        "    concat_sent_trans = Concatenate(axis=1)([sentence, translation])\n",
        "\n",
        "    dense_in = concat_sent_trans\n",
        "    for layer in layers:\n",
        "        dense_layer = L.Dense(layer, activation=activation)(dense_in)\n",
        "        dense_in = dense_layer\n",
        "    output = L.Dense(1)(dense_layer)\n",
        "\n",
        "    model = Model([sentence, translation], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy6o7KPd0lKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_features_model(architecture):\n",
        "    layers = architecture['layers']\n",
        "    activation = architecture['activation']\n",
        "    num_features = architecture['num_features_sent']\n",
        "    num_features_trans = architecture.get('num_features_trans', num_features)\n",
        "\n",
        "    # Inputs are sentence embeddings\n",
        "    sentence = L.Input(shape=(num_features,), name='sentence')\n",
        "    translation = L.Input(shape=(num_features_trans,), name='translation')\n",
        "\n",
        "    concat_sent_trans = Concatenate(axis=1)([sentence, translation])\n",
        "\n",
        "    dense_in = concat_sent_trans\n",
        "    for layer in layers:\n",
        "        dense_layer = L.Dense(layer, activation=activation)(dense_in)\n",
        "        dense_in = dense_layer\n",
        "    output = L.Dense(1)(dense_layer)\n",
        "\n",
        "    model = Model([sentence, translation], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSKXI6erlhHY",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3MnWY86j7I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gru_model(architecture):\n",
        "    gru_units = architecture['gru_units']\n",
        "    layers = architecture['layers']\n",
        "    activation = architecture['activation']\n",
        "    num_features = architecture['num_features']\n",
        "\n",
        "    # Inputs are word embeddings\n",
        "    sentence = L.Input(shape=(None, num_features), name='sentence')\n",
        "    translation = L.Input(shape=(None, num_features), name='translation')\n",
        "\n",
        "    masked_sent = Masking()(sentence)\n",
        "    masked_sent = Bidirectional(GRU(units=gru_units, return_sequences=True))(masked_sent)\n",
        "    masked_sent = Bidirectional(GRU(units=gru_units, return_sequences=False))(masked_sent)\n",
        "    \n",
        "    masked_trans = Masking()(translation)\n",
        "    masked_trans = Bidirectional(GRU(units=gru_units, return_sequences=True))(masked_trans)\n",
        "    masked_trans = Bidirectional(GRU(units=gru_units, return_sequences=False))(masked_trans)\n",
        "\n",
        "    concat_sent_trans = Concatenate(axis=1)([masked_sent, masked_trans])\n",
        "\n",
        "    dense_in = concat_sent_trans\n",
        "    for layer in layers:\n",
        "        dense_layer = L.Dense(layer, activation=activation)(dense_in)\n",
        "        dense_in = dense_layer\n",
        "    output = L.Dense(1)(dense_layer)\n",
        "\n",
        "    model = Model([sentence, translation], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrzYuvxLJ7v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_lstm_model(architecture):\n",
        "    lstm_units = architecture['lstm_units']\n",
        "    layers = architecture['layers']\n",
        "    activation = architecture['activation']\n",
        "    num_features = architecture['num_features']\n",
        "\n",
        "    # Inputs are word embeddings\n",
        "    sentence = L.Input(shape=(None, num_features), name='sentence')\n",
        "    translation = L.Input(shape=(None, num_features), name='translation')\n",
        "\n",
        "    masked_sent = Masking()(sentence)\n",
        "    masked_sent = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(masked_sent)\n",
        "    masked_sent = Bidirectional(LSTM(units=lstm_units, return_sequences=False))(masked_sent)\n",
        "    \n",
        "    masked_trans = Masking()(translation)\n",
        "    masked_trans = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(masked_trans)\n",
        "    masked_trans = Bidirectional(LSTM(units=lstm_units, return_sequences=False))(masked_trans)\n",
        "\n",
        "\n",
        "    concat_sent_trans = Concatenate(axis=1)([masked_sent, masked_trans])\n",
        "\n",
        "    dense_in = concat_sent_trans\n",
        "    for layer in layers:\n",
        "        dense_layer = L.Dense(layer, activation=activation)(dense_in)\n",
        "        dense_in = dense_layer\n",
        "    output = L.Dense(1)(dense_layer)\n",
        "\n",
        "    model = Model([sentence, translation], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87-D1QFflOfO",
        "colab_type": "text"
      },
      "source": [
        "### Attention models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBkJTu4RmDso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_attention_model(architecture):\n",
        "    lstm_units = architecture['lstm_units']\n",
        "    layers = architecture['layers']\n",
        "    activation = architecture['activation']\n",
        "    num_features = architecture['num_features']\n",
        "\n",
        "    sentence = L.Input(shape=(None, num_features), name='sentence')\n",
        "    translation = L.Input(shape=(None, num_features), name='translation')\n",
        "\n",
        "    attention_sent = attention(sentence, key_size=300, val_size=num_features)\n",
        "    sent_summary = Bidirectional(LSTM(units=lstm_units, return_sequences=True, dropout=0.01))(attention_sent)\n",
        "    sent_summary = Bidirectional(LSTM(units=lstm_units, return_sequences=False, dropout=0.01))(sent_summary)\n",
        "    \n",
        "    attention_trans = attention(translation, key_size=300, val_size=num_features)\n",
        "    trans_summary = Bidirectional(LSTM(units=lstm_units, return_sequences=True, dropout=0.01))(attention_trans)\n",
        "    trans_summary = Bidirectional(LSTM(units=lstm_units, return_sequences=False, dropout=0.01))(trans_summary)\n",
        "    \n",
        "    concat_sent_trans = Concatenate(axis=1)([sent_summary, trans_summary])\n",
        "\n",
        "    dense_in = concat_sent_trans\n",
        "    for layer in layers:\n",
        "        dense_layer = L.Dense(layer, activation=activation)(dense_in)\n",
        "        dense_in = dense_layer\n",
        "    output = L.Dense(1)(dense_layer)\n",
        "\n",
        "    model = Model([sentence, translation], output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def attention(hidden_states, key_size=20, val_size=300):\n",
        "    # hidden_states: (batch, seq_len, width)\n",
        "    print(\"hidden\", hidden_states.shape)\n",
        "    att_key = L.Dense(key_size)(hidden_states)\n",
        "    # att_key: (batch, seq_len, key_size)\n",
        "    print(\"key\",  att_key.shape)\n",
        "    att_q = L.Dense(key_size)(hidden_states)\n",
        "    # att_q: (batch, seq_len, key_size)\n",
        "    print(\"query\", att_q.shape)\n",
        "    att_w = L.Lambda(lambda key: K.batch_dot(key, att_q, axes=[2,2]))(att_key)\n",
        "    # K.batch_dot(att_key, att_q, axes=[2,2])\n",
        "    att_w = L.Softmax()(att_w)\n",
        "    # att_w: (batch, seq_len, seq_len)\n",
        "    print(\"weights\", att_w.shape)\n",
        "    att_v = L.Dense(val_size)(hidden_states)\n",
        "    # att_v: (batch, seq_len, val_size)\n",
        "    print(\"values\", att_v.shape)\n",
        "    att_out = L.Lambda(lambda weights: K.batch_dot(weights, att_v))(att_w)\n",
        "    # att_out: (batch, seq_len, val_size)\n",
        "    print(\"output\", att_out.shape)\n",
        "    return att_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-IZhB2HeY_9",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "The following cells define the classes and functions to train models and evaluate their performance. We also define a function for hyper-parameter tuning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-g8QC6nezI5",
        "colab_type": "text"
      },
      "source": [
        "Define `Sequence` class to split dataset into batches for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3RRXpRE9wwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputSequence(Sequence):\n",
        "    def __init__(self, sentences, translations, scores, batch_size):\n",
        "        self.sentences = sentences\n",
        "        self.translations = translations\n",
        "        self.scores = scores\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.total_batch_size = len(self.sentences)\n",
        "        self.indexes = np.arange(len(self.sentences))\n",
        "        # self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns number of batches\"\"\"\n",
        "        return int(np.ceil(self.total_batch_size / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns the tuple containing list of sentence and translation\n",
        "        tensors and the expected scores for the translations\"\"\"\n",
        "\n",
        "        start_idx = index * self.batch_size\n",
        "        end_idx = min(start_idx + self.batch_size, self.total_batch_size)\n",
        "        current_batch_size = end_idx - start_idx\n",
        "\n",
        "        # Get slice with indexes for batch\n",
        "        batch_idx_slice = self.indexes[start_idx: end_idx]\n",
        "\n",
        "        sentences_batch = self.sentences[batch_idx_slice]\n",
        "        translations_batch = self.translations[batch_idx_slice]\n",
        "        scores_batch = self.scores[batch_idx_slice]\n",
        "\n",
        "        return [sentences_batch, translations_batch], scores_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNOjvQVu6vyy",
        "colab_type": "text"
      },
      "source": [
        "Define utilities to load datasets for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOf1Dbz-pz-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_and_split(d1, d2, indices=None, size=7000):\n",
        "    \"\"\"\n",
        "    Combines two datasets, shuffles them and returns two partitions of \n",
        "    the same sizes as the inputs, returning the indices used for the shuffling.\n",
        "\n",
        "    If indices are not provided, then a random shuffle is applied. Otherwise, \n",
        "    the provided indices are used for the shuffle\n",
        "    \"\"\"\n",
        "    if indices is None:\n",
        "        indices = np.arange(len(d1) + len(d2))\n",
        "        np.random.shuffle(indices)\n",
        "    all_data = np.concatenate((d1, d2), axis=0)\n",
        "    d1_indices, d2_indices = indices[:size], indices[size:]\n",
        "    return all_data[d1_indices], all_data[d2_indices], indices\n",
        "\n",
        "def get_train_data(embeddings_model=\"spacy\", avg_word_embeddings=False, shuffle=False):\n",
        "    train_sentences, train_translations = load_inputs(dataset=\"train\", model=embeddings_model)\n",
        "    test_sentences, test_translations = load_inputs(dataset=\"test\", model=embeddings_model)\n",
        "    train_scores = load_scores('data/en-de/train.ende.scores')\n",
        "    test_scores = load_scores('data/en-de/dev.ende.scores')\n",
        "\n",
        "    if avg_word_embeddings:\n",
        "        # Make sure embeddings are word embeddings, not sentence embeddings\n",
        "        train_sentences = np.average(train_sentences, axis=1)\n",
        "        train_translations = np.average(train_translations, axis=1)\n",
        "        test_sentences = np.average(test_sentences, axis=1)\n",
        "        test_translations = np.average(test_translations, axis=1)\n",
        "    \n",
        "    if shuffle:\n",
        "        train_sentences, test_sentences, indices = shuffle_and_split(train_sentences, test_sentences)\n",
        "        train_translations, test_translations, _ = shuffle_and_split(train_translations, test_translations, indices=indices)\n",
        "        train_scores, test_scores, _ = shuffle_and_split(train_scores, test_scores, indices=indices)\n",
        "      \n",
        "    return (train_sentences, train_translations, train_scores), (test_sentences, test_translations, test_scores)\n",
        "\n",
        "def get_train_sequences(train_data, val_data, batch_size):\n",
        "    train_sentences, train_translations, train_scores = train_data\n",
        "    test_sentences, test_translations, test_scores = val_data\n",
        "\n",
        "    train_seq = InputSequence(train_sentences,\n",
        "                                    train_translations,\n",
        "                                    train_scores,\n",
        "                                    batch_size)\n",
        "\n",
        "    val_seq = InputSequence(test_sentences,\n",
        "                                test_translations,\n",
        "                                test_scores,\n",
        "                                batch_size)\n",
        "    \n",
        "    return train_seq, val_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZU-lcfsfl_0",
        "colab_type": "text"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "We define below a function to train a model using training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpp_mjK8-Uog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(model, train_seq, val_seq, n_epochs):\n",
        "    \"\"\"Trains the model\"\"\"\n",
        "    with tf.device('/GPU:0'):\n",
        "        callbacks = [TerminateOnNaN(),\n",
        "                    ModelCheckpoint(\"saved_models/model_best.hdf5\",\n",
        "                                    monitor='val_loss', verbose=1,\n",
        "                                    save_best_only=True, save_weights_only=True),\n",
        "                    ModelCheckpoint(\"saved_models/model.hdf5\", verbose=0,\n",
        "                                    save_best_only=False, save_weights_only=True),\n",
        "                    EarlyStopping(monitor='val_loss', patience=7, verbose=1,\n",
        "                                restore_best_weights=True),\n",
        "                    TensorBoard(log_dir=\"saved_models/logs\", write_images=True)]\n",
        "\n",
        "        model.fit_generator(generator=train_seq, epochs=n_epochs,\n",
        "                            validation_data=val_seq, shuffle=True,\n",
        "                            callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k9bsRAjhGVn",
        "colab_type": "text"
      },
      "source": [
        "We define below function to carry out k-fold cross-validation on a neural network model with a particular architecture to evaluate its performance. We split the training data into training and validation dataset over 7 folds. The validation dataset is used to monitor the model's performance on an unseen dataset, early stopping when the validation loss starts increasing. The trained models are tested on the test dataset provided (usually the dev dataset). \n",
        "\n",
        "A new neural network is constructed at the beginning of each fold, using the `build_model` function and the neural network parameters `params`. Tunable parameters for the model depend on its architecture, but some of the ones we used include:\n",
        "*    `layers`: list containing the number of outputs for hidden `Dense` layers in the network.\n",
        "*    `num_features`: number of input features for the network.\n",
        "*    `activation`: activation function to use in the `Dense` layers.\n",
        "*    `gru_units`/`lstm_units`: dimension of the hidden state of the recurrent layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yjLJ4yhduVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_cross_validation(build_model, params, train_data, test_data, lr, batch_size, indices=None, shuffle=False):\n",
        "    pearson_scores = []\n",
        "    mae_scores = []\n",
        "    mse_scores = []\n",
        "\n",
        "    best_score = 0.0\n",
        "    \n",
        "    n_epochs = 25\n",
        "\n",
        "    train_sentences, train_translations, train_scores = train_data\n",
        "    test_sentences, test_translations, test_scores = test_data\n",
        "\n",
        "    if shuffle:\n",
        "        train_sentences, test_sentences, indices = shuffle_and_split(train_sentences, test_sentences)\n",
        "        train_translations, test_translations, _ = shuffle_and_split(train_translations, test_translations, indices=indices)\n",
        "        train_scores, test_scores, _ = shuffle_and_split(train_scores, test_scores, indices=indices)\n",
        "\n",
        "    train_data_indexes = np.arange(len(train_sentences))\n",
        "\n",
        "    k_fold = KFold(n_splits=7, shuffle=True)\n",
        "    \n",
        "    test_seq = InputSequence(test_sentences,\n",
        "                            test_translations,\n",
        "                            test_scores,\n",
        "                            batch_size)\n",
        "\n",
        "    for idx, (train_index, test_index) in enumerate(k_fold.split(train_data_indexes, train_data_indexes)):\n",
        "        # if idx % 2 == 0:\n",
        "        #     continue\n",
        "        # print(f\"Fold {(idx + 1) // 2} / 7\")\n",
        "        print(f\"Fold {idx + 1} / 7\")\n",
        "        X_train = (train_sentences[train_index], train_translations[train_index])\n",
        "        X_val = (train_sentences[test_index], train_translations[test_index])\n",
        "        y_train = train_scores[train_index]\n",
        "        y_val = train_scores[test_index]\n",
        "        \n",
        "        print(params)\n",
        "        model = build_model(params)\n",
        "        model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss='mean_squared_error',\n",
        "                    metrics=['mae'])\n",
        "        \n",
        "        train_seq, val_seq = get_train_sequences(X_train + (y_train,), X_val + (y_val,), batch_size)\n",
        "\n",
        "        fit_model(model, train_seq, val_seq, n_epochs)\n",
        "\n",
        "        predicted_scores = model.predict(test_seq)\n",
        "        mae_score = mean_absolute_error(test_scores, predicted_scores.squeeze())\n",
        "        print(f'\\nAverage MAE: {mae_score}')\n",
        "        pearson_score = pearsonr(test_scores, predicted_scores.squeeze())\n",
        "        print(f'Average Pearson: {pearson_score}')\n",
        "        mse_score = (mean_squared_error(test_scores, predicted_scores.squeeze())) ** (0.5)\n",
        "        print(f'Average RMSE: {mse_score}')\n",
        "        pearson_scores.append(pearson_score)\n",
        "        mae_scores.append(mae_score)\n",
        "        mse_scores.append(mse_score)\n",
        "        \n",
        "    return pearson_scores, mae_scores, mse_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-MgRGWx01Qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example cross-validation function call for MLP neural network\n",
        "\n",
        "# These parameters can be changed depending on which model you are building.\n",
        "architecture = {'num_features': 300, 'layers': [100], 'activation': 'relu'}\n",
        "(train_data, test_data) = get_train_data(avg_word_embeddings=True)\n",
        "learning_rate = 0.0005\n",
        "batch_size = 50\n",
        "\n",
        "perform_cross_validation(build_mlp_model, architecture, train_data, test_data, learning_rate, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc_0c94ghY12",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter tuning\n",
        "\n",
        "The `parameter_search` method allows you to perform hyperparameter tuning. The parameters to the method are as follows:\n",
        "\n",
        "\n",
        "*   `build_model`: Function to construct the model.\n",
        "*   `parameters`: Tunable hyperparameters for training such as batch_size and learning_rate.\n",
        "*   `architecture`: Tunable hyperparameters used to change the architecture of the model.\n",
        "*   `embeddings_model`: The embeddings model to be used - either `spacy`, `word2vec` or `bert`.\n",
        "*   `avg_word_embeddings`: Whether or not the word embeddings should be averaged to form an averaged sentence embedding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toriJsPg_BV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameter_search(build_model, parameters, architecture, embeddings_model='spacy', avg_word_embeddings=False):\n",
        "    train_data, val_data = get_train_data(embeddings_model=embeddings_model, avg_word_embeddings=avg_word_embeddings)\n",
        "\n",
        "    n_epochs = 10\n",
        "    sampled_arch_params = list(ParameterSampler(architecture, n_iter=20, random_state=42)) \n",
        "    parameters['architecture'] = sampled_arch_params\n",
        "\n",
        "    sampled_params = list(ParameterSampler(parameters, n_iter=7, random_state=42))\n",
        "    \n",
        "    models = []\n",
        "    scores = []\n",
        "    best_model = None\n",
        "    best_score = 0.0\n",
        "    for i, params in enumerate(sampled_params):\n",
        "        print(f'Parameter set {i + 1}')\n",
        "        print(params)\n",
        "\n",
        "        pearson, mae, mse = perform_cross_validation(build_model, params['architecture'], train_data, val_data, params['learning_rate'], params['batch_size'], shuffle=True)\n",
        "        mae_score = np.average(mae)\n",
        "        mse_score = np.average(mse)\n",
        "        pearson_score = np.average(pearson, axis=0)\n",
        "\n",
        "        print(f'MAE: {mae_score}')\n",
        "        print(f'Pearson: {pearson_score}')\n",
        "        print(f'MSE: {mse_score}')\n",
        "        if (pearson_score[0] > best_score):\n",
        "            best_score = pearson_score[0]\n",
        "        scores.append((mae_score, mse_score, pearson_score))\n",
        "\n",
        "    return best_score, scores, sampled_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gLK4Zge9PLz",
        "colab_type": "text"
      },
      "source": [
        "We define below functions to perform hyperparameter tuning on the different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBXpYMoO_Mpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameter_search_mlp():\n",
        "    parameters = {\n",
        "        'learning_rate': [0.001, 0.0005, 0.002],\n",
        "        'batch_size': [32, 64, 50, 100],\n",
        "    }\n",
        "\n",
        "    architecture = {\n",
        "        'layers': [[50], [50, 100], [25, 100], [150], [512, 256, 256], [256, 128, 128, 256],[100, 100, 50, 50], [400], [75, 50, 25], [100, 100], [100, 200], [400, 200, 100], [150, 100, 50], [200, 100, 100], [100, 50]], \n",
        "        'activation': [\"relu\", \"tanh\"],\n",
        "        'num_features': [300]\n",
        "    }\n",
        "        \n",
        "    return parameter_search(build_mlp_model, parameters, architecture, avg_word_embeddings=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA9xnaYxeSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameter_search_features():\n",
        "    parameters = {\n",
        "        'learning_rate': [0.001, 0.0005, 0.002],\n",
        "        'batch_size': [32, 64, 50, 100],\n",
        "    }\n",
        "\n",
        "    architecture = {\n",
        "        'layers': [[512, 256, 256], [256, 128, 128, 256],[100, 100, 50, 50], [400], [75, 50, 25], [100, 100], [100, 200], [400, 200, 100], [150, 100, 50], [200, 100, 100], [100, 50]], \n",
        "        'activation': [\"relu\", \"tanh\"],\n",
        "        'num_features_sent': [141],\n",
        "        'num_features_trans': [126],\n",
        "    }\n",
        "        \n",
        "    return parameter_search(build_features_model, parameters, architecture, embeddings_model='features', avg_word_embeddings=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-EB9YZCSNYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameter_search_gru():\n",
        "    parameters = {\n",
        "        'learning_rate': [0.001, 0.0005, 0.002],\n",
        "        'batch_size': [32, 64, 50, 100],\n",
        "    }\n",
        "\n",
        "    architecture = {\n",
        "        'gru_units': [64, 32, 128, 96], \n",
        "        'layers': [[512, 256, 256], [256, 128, 128, 256],[100, 100, 50, 50], [400], [75, 50, 25], [100, 100], [100, 200], [400, 200, 100], [150, 100, 50], [200, 100, 100], [100, 50]], \n",
        "        'activation': [\"relu\", \"tanh\"],\n",
        "        'num_features': [300]\n",
        "    }\n",
        "        \n",
        "    return parameter_search(build_gru_model, parameters, architecture) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEpmTcxUKszl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameter_search_lstm():\n",
        "    parameters = {\n",
        "        'learning_rate': [0.001, 0.0005, 0.002],\n",
        "        'batch_size': [32, 64, 50, 100],\n",
        "    }\n",
        "\n",
        "    architecture = {\n",
        "        'lstm_units': [64, 32, 128, 96], \n",
        "        'layers': [[512, 256, 256], [256, 128, 128, 256],[100, 100, 50, 50], [400], [75, 50, 25], [100, 100], [100, 200], [400, 200, 100], [150, 100, 50], [200, 100, 100], [100, 50]], \n",
        "        'activation': [\"relu\", \"tanh\"],\n",
        "        'num_features': [300]\n",
        "    }\n",
        "\n",
        "    return parameter_search(build_gru_model, parameters, architecture) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqfsyQvZSqLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameter_search_attention(use_cosine=False):\n",
        "    parameters = {\n",
        "        'learning_rate': [0.001, 0.0005, 0.002],\n",
        "        'batch_size': [32, 64, 50, 100],\n",
        "    }\n",
        "\n",
        "    architecture = {\n",
        "        'gru_units': [64, 32, 128, 92],\n",
        "        'layers': [[512, 256, 256], [256, 128, 128, 256],[100, 100, 50, 50], [400], [75, 50, 25], [100, 100], [100, 200], [400, 200, 100], [150, 100, 50], [200, 100, 100], [100, 50]], \n",
        "        'activation': [\"relu\", \"tanh\"],\n",
        "        'num_features': [300]\n",
        "    }\n",
        "    \n",
        "    return parameter_search(build_attention_model, parameters, architecture) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSWQjO5HFIkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of hyperparameter tuning call\n",
        "parameter_search_mlp()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovyn0VeLNm8r",
        "colab_type": "text"
      },
      "source": [
        "# Running model on Test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvHC3-yYPyo7",
        "colab_type": "text"
      },
      "source": [
        "Build model with best architecture and train it on both training datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3R-4LLlYXqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_predictions(build_model, params, lr, embedding_model='spacy', avg_word_embeddings=False):\n",
        "    train_data, val_data = get_train_data(embeddings_model=embedding_model, avg_word_embeddings=avg_word_embeddings)\n",
        "\n",
        "    test_sentences, test_translations = load_inputs(dataset='test', model=embedding_model)\n",
        "\n",
        "    if avg_word_embeddings:\n",
        "        test_sentences = np.average(test_sentences, axis=1)\n",
        "        test_translations = np.average(test_translations, axis=1)\n",
        "\n",
        "    n_epochs = 2\n",
        "\n",
        "    model = build_model(params)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss='mean_squared_error',\n",
        "                metrics=['mae'])\n",
        "    \n",
        "    train_sentences, train_translations, train_scores = train_data\n",
        "    val_sentences, val_translations, val_scores = val_data\n",
        "\n",
        "    print(train_sentences.shape)\n",
        "    print(val_sentences.shape)\n",
        "    train_sentences = np.concatenate((train_sentences, val_sentences), axis=0)\n",
        "    train_translations = np.concatenate((train_translations, val_translations), axis=0)\n",
        "    train_scores = np.concatenate((train_scores, val_scores), axis=0)\n",
        "\n",
        "    train_seq = InputSequence(train_sentences,\n",
        "                                    train_translations,\n",
        "                                    train_scores,\n",
        "                                    64)\n",
        "    \n",
        "    test_seq = InputSequence(test_sentences,\n",
        "                                    test_translations,\n",
        "                                    np.arange(len(test_translations)),\n",
        "                                    8000)\n",
        "\n",
        "    model.fit_generator(generator=train_seq, epochs=n_epochs, shuffle=True)\n",
        "\n",
        "    predicted_scores = model.predict(test_seq)\n",
        "    return predicted_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rj-Jou1KtQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_test_predictions(predicted_scores):\n",
        "    with open(\"predictions.txt\", \"w\") as f:\n",
        "        for score in predicted_scores:\n",
        "            f.write(f\"{score[0]}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuQLSyK7YbAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of testing the custom features\n",
        "architecture = {'num_features_sent': 141, 'num_features_trans': 126, 'layers': [400], 'activation': 'tanh'}\n",
        "predicted_scores = get_test_predictions(build_features_model, architecture, 0.002, embedding_model='features', avg_word_embeddings=False)\n",
        "\n",
        "write_test_predictions(predicted_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}